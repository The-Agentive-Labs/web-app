<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Usability Testing | Design Research Library</title>
  <link rel="stylesheet" href="design-system.css">
  <link rel="stylesheet" href="pattern-page.css">
</head>
<body>
  <div class="research-layout">
    <aside class="sidebar">
      <div class="sidebar-title"><a href="research-library.html">Research Library</a></div>
      <nav class="nav-section active">
        <div class="nav-section-title">Execution & Testing<span class="expand-icon">▸</span></div>
        <ul class="nav-links">
          <li><a href="low-fidelity.html">Low-Fidelity Design</a></li>
          <li><a href="prototyping.html">Prototyping</a></li>
          <li><a href="design-guide.html">Design Guide</a></li>
          <li><a href="testing.html" class="active">Testing</a></li>
        </ul>
      </nav>
    </aside>

    <main class="main-content">
      <header class="page-header">
        <div class="capability-tags">
          <span class="tag">Testing</span>
          <span class="tag">Validation</span>
        </div>
        <h1>Usability Testing</h1>
        <p class="purpose">Observe users interacting with your product or prototype to identify usability issues and validate design decisions.</p>
      </header>

      <div class="pattern-split">
        <section class="learn-container">
          <h2>Learn</h2>
          
          <h3>What is Usability Testing?</h3>
          <p>Usability Testing is a method where you observe real users attempting to complete tasks with your product, prototype, or design. The goal: identify where users get stuck, confused, or frustrated—and fix those issues before launch. It's the reality check that transforms assumptions into validated designs.</p>

          <h3>Why Test?</h3>
          <ul>
            <li><strong>Catch Problems Early:</strong> Fix usability issues in prototypes, not production. Cheaper and faster.</li>
            <li><strong>See What Users Actually Do:</strong> What users say they'll do ≠ what they actually do. Testing reveals truth.</li>
            <li><strong>Validate Design Decisions:</strong> Gut feelings don't ship products. User behavior does.</li>
            <li><strong>Build Confidence:</strong> Stakeholders trust designs that pass real user scrutiny.</li>
          </ul>

          <h3>Types of Usability Testing</h3>
          <ul>
            <li><strong>Moderated:</strong> Researcher facilitates in real-time (in-person or remote). Best for exploring "why" users struggle.</li>
            <li><strong>Unmoderated:</strong> Users complete tasks independently, recorded for later review. Faster, scales better.</li>
            <li><strong>Formative:</strong> Early-stage testing to identify problems. Goal: learn and iterate.</li>
            <li><strong>Summative:</strong> Late-stage testing to validate readiness. Goal: measure success metrics before launch.</li>
          </ul>

          <h3>Core Metrics</h3>
          <ul>
            <li><strong>Task Success Rate:</strong> % of users who complete tasks without help</li>
            <li><strong>Time on Task:</strong> How long it takes to complete tasks</li>
            <li><strong>Error Rate:</strong> Frequency of user mistakes</li>
            <li><strong>Satisfaction:</strong> Self-reported ease of use (e.g., SUS score)</li>
          </ul>

          <h3>Best Practices</h3>
          <ul>
            <li><strong>Test with 5-8 Users:</strong> Nielsen research shows 5 users reveal 85% of usability issues. Diminishing returns after that.</li>
            <li><strong>Use Realistic Tasks:</strong> Give users scenarios, not instructions. "Find and book a flight" beats "Click the search button."</li>
            <li><strong>Think Aloud Protocol:</strong> Ask users to narrate their thought process. "What are you thinking?" "What would you do next?"</li>
            <li><strong>Don't Intervene:</strong> Let users struggle. Their mistakes reveal design flaws. Only help if they're completely stuck.</li>
            <li><strong>Record Sessions:</strong> Capture screen + audio. Review later to catch details you missed live.</li>
            <li><strong>Test Iteratively:</strong> Fix major issues, then test again. Repeat until task success rate hits 90%+.</li>
          </ul>
        </section>

        <section class="build-container">
          <h2>Build</h2>

          <h3>Template: Test Plan</h3>
          <div class="code-block">
            <code><strong>Test Goals</strong><br>
What are we testing? [Prototype, feature, flow]<br>
What questions do we need answered?<br>
<br>
<strong>Tasks</strong><br>
1. [Task description + success criteria]<br>
2. [Task 2...]<br>
<br>
<strong>Participants</strong><br>
Who: [Target user segment]<br>
How Many: [5-8 for qualitative; 20+ for quantitative]<br>
<br>
<strong>Metrics</strong><br>
- Task success rate<br>
- Time on task<br>
- Error rate<br>
- Satisfaction (1-5 scale)<br>
<br>
<strong>Scenarios</strong><br>
[Set context for each task. Give users a "why."]</code>
          </div>

          <h3>Step-by-Step Instructions</h3>
          <ol>
            <li><strong>Define What You're Testing:</strong> A specific flow? The overall product? Be clear on scope.</li>
            <li><strong>Write Task Scenarios:</strong> Create 3-5 realistic tasks users would actually do. Frame as goals, not instructions.</li>
            <li><strong>Recruit Participants:</strong> Target your actual user segment. Use screeners to filter. Recruit 5-8 for moderated tests.</li>
            <li><strong>Prepare Your Prototype:</strong> Ensure it's functional enough to test the critical path. Fix obvious bugs first.</li>
            <li><strong>Run a Pilot:</strong> Test with 1 colleague to refine tasks and timing. Adjust as needed.</li>
            <li><strong>Conduct Tests:</strong> Introduce the test. Give users the first task. Observe. Take notes. Don't interrupt. Ask follow-up questions after each task.</li>
            <li><strong>Analyze Results:</strong> Calculate success rates, identify patterns in errors. Cluster issues by severity (critical, major, minor).</li>
            <li><strong>Prioritize Fixes:</strong> Fix critical issues (task blockers) first. Iterate and retest.</li>
          </ol>

          <h3>Example: Freelance Invoice App</h3>
          <div class="example-box">
            <p><strong>Test Goal:</strong> Validate that new users can create and send their first invoice in under 2 minutes without assistance.</p>
            
            <p><strong>Tasks:</strong></p>
            <ol>
              <li><strong>Task 1:</strong> You just finished a logo design for a client, Sarah Lee. Create an invoice for $500, due in 30 days, and send it to sarah@example.com.</li>
              <li><strong>Task 2:</strong> Check the status of the invoice you just sent.</li>
            </ol>

            <p><strong>Participants:</strong> 6 freelance designers/developers, 1-3 years experience</p>

            <p><strong>Results:</strong></p>
            <ul>
              <li><strong>Task 1 Success:</strong> 5/6 users (83%) completed without help</li>
              <li><strong>Avg Time:</strong> 1m 45s (meets <2min goal)</li>
              <li><strong>Issue Found:</strong> 3 users struggled to find the "Send" button (placed too low on form). <strong>Fix:</strong> Move "Send" to sticky footer.</li>
            </ul>

            <p><strong>Action:</strong> Fix button placement. Retest with 3 new users. Target: 100% success rate.</p>
          </div>
        </section>
      </div>

      <section class="ai-copilot">
        <h2>AI + Human Collaboration</h2>
        
        <div class="comparison-table">
          <div class="comparison-column">
            <h4>Human Value</h4>
            <ul>
              <li>Empathy to understand why users struggle</li>
              <li>Active observation to catch non-verbal cues (frustration, confusion)</li>
              <li>Contextual judgment to assess severity of usability issues</li>
              <li>Strategic prioritization of which issues to fix first</li>
            </ul>
          </div>
          <div class="comparison-column">
            <h4>AI Value</h4>
            <ul>
              <li>Automated analysis of heatmaps and click patterns</li>
              <li>Transcription and tagging of session recordings</li>
              <li>Calculation of metrics (success rate, time on task)</li>
              <li>Pattern identification across multiple test sessions</li>
            </ul>
          </div>
        </div>

        <h3>Key Considerations</h3>
        <ul>
          <li><strong>AI for Analytics:</strong> Use AI to analyze quantitative data (click paths, time on task). But humans must interpret what these patterns mean for design.</li>
          <li><strong>Session Review:</strong> AI can tag moments of hesitation or errors in recordings. Humans must watch to understand the "why."</li>
          <li><strong>No AI as Moderator:</strong> Usability testing requires human empathy and adaptability. AI cannot probe deeper or adjust questions in real-time.</li>
        </ul>

        <h3>"AI in the Loop" Model</h3>
        <ol>
          <li><strong>Test Setup:</strong> AI suggests task scenarios based on user flows. Humans refine for realism and clarity.</li>
          <li><strong>Session Recording:</strong> AI transcribes user think-aloud commentary and tags friction moments. Humans review to understand root causes.</li>
          <li><strong>Analysis:</strong> AI calculates metrics and identifies common error patterns. Humans prioritize fixes based on user impact and technical feasibility.</li>
        </ol>

        <h3>Watch-Outs</h3>
        <ul>
          <li>Don't let AI conduct tests. Usability testing requires human judgment, empathy, and the ability to ask follow-up questions.</li>
          <li>Avoid over-reliance on heatmaps. They show where users click, not why. Combine with qualitative observation.</li>
          <li>Be cautious of AI-generated insights. "Users spent 10s on this button" doesn't tell you if that's good or bad. Context matters.</li>
          <li>Privacy: Ensure AI analysis tools comply with data protection laws. Always inform participants if AI is involved.</li>
        </ul>
      </section>
    </main>
  </div>

  <script src="sidebar.js"></script>
</body>
</html>